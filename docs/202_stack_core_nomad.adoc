=== Nomad Reference Architecture
https://developer.hashicorp.com/nomad/tutorials/enterprise/production-reference-architecture-vm-with-consul#ra

=== Nomad Deployment Guide
https://developer.hashicorp.com/nomad/tutorials/enterprise/production-deployment-guide-vm-with-consul

=== Networking
In nomad client config is a `network` stanza.
You can bin there some interfaces to a named physical or virtual betwork on your host.

.Bind two named networks
[source,hcl]
----
 host_network "private" {
   interface = "eth0"
   #cidr = "203.0.113.0/24"
   #reserved_ports = "22,80"
}

host_network "public" {
    interface = "eth1"
}
----

After that you can bind in a job stanza the service to a specific interface

.Bind service to specific interface. For example make http availiable only through private network
[source,hcl]
----
  network {
    port "http-priv" {
      static       = 80
      host_network = "private"
    }
    port "https-priv" {
      static       = 443
      host_network = "private"
    }

    port "https-pub" {
      static       = 443
      host_network = "public"
    }
  }
----


=== Memory and cpu limits
There are two main types of setting resources in nomad. The first in the nomad client configuration and the second is inside in the job configuration.

The resource configuration in the client configuration reserves memory and cpu for operating nomad. after this reservation nomad subtracts this from the available resources for job scheduling.

The resource configuration in the job configuration limits the job itself. If you don't enable https://developer.hashicorp.com/nomad/docs/job-specification/resources#memory-oversubscription[Memory Oversubscription] then the memory you defined is the min and max limit at the same time.

=== Job resiliency configuration

There are several mechanisms in nomad to control the schedule errors.

==== reschedule
job -> reschedule  | job -> group -> reschedule
This handles the case where the specified number of restarts have been attempted and the task still isn’t running. This suggests the issue could be with the Nomad client such as a hardware failure or kernel deadlock. The reschedule stanza is used to specify details for rescheduling a failing task to another nomad client. For example, reschedule the task group an unlimited number of times and increase the delay between subsequent attempts exponentially, with a starting delay of 30 seconds up to a maximum of 1 hour.

==== restart
job -> group -> restart | job -> group -> task -> restart

Specifies strategy for Nomad to restart failed tasks on the same nomad client. For example, if the application server has crashed, attempt 2 restarts within 30 minutes, delay 15s between each restart, and don’t try anymore restarts after those are exhausted.

* `delay` - Instructs the client to wait until another interval before restarting the task.

* `fail`- Instructs the client not to attempt to restart the task once the number of attempts have been used. This is the default behavior. This mode is useful for non-idempotent jobs which are unlikely to succeed after a few failures. The allocation will be marked as failed and the scheduler will attempt to reschedule the allocation according to the reschedule block.

==== check
job -> group -> service -> check |
job -> group -> task -> service -> check

The check block instructs Nomad to register a check associated with a service into the Nomad or Consul service provider.

==== check_restart
job -> group -> task -> service -> check_restart | job -> group -> task -> service -> check -> check_restart
Specifies how Nomad should restart a task that is not yet failing, but has become unresponsive or otherwise unhealthy. Works together with Consul health checks. Nomad restarts tasks when a health check has failed. For example, restart the Redis task after its health check has failed 3 consecutive times, and wait 90 seconds after restarting the task to resume health checking.

==== update
job -> update  | job -> group -> update
Specifies update strategy Nomad uses when deploying a new version of the task group. i.e. when nomad job run path/to/jobspec is run. For example, perform rolling updates 3 at a time and wait until all tasks for an allocation are running and their Consul health checks are passing for at least 10 seconds before considering the allocation healthy.

==== migrate
When a Nomad client needs to come out of service, it gets marked for draining and tasks will no longer be scheduled on it. Then Nomad will migrate all existing jobs to other clients. The migrate stanza specifies the strategy for migrating tasks off of draining nodes. For example, migrate one allocation at a time, and mark migrated allocations healthy once all their tasks are running and associated health checks are passing for 10 seconds or more within a 5 minute deadline.


==== Wrap it up

. Define a reschedule for whole job.
+
.reschedule for job definition unlimited
[source,hcl]
----
reschedule {
  delay          = "10s"
  delay_function = "constant"
  unlimited      = true
}
----
This instructs nomad to try infinite schedule a failed task on the same node.
If a task have a dynamic volume or is stales you can set another  limit for instruct the scheduler to schedule the task on another node.

+
.reschedule job on another node failed deployment when more than 3 times failed
[source,hcl]
----
reschedule {
  attempts       = 3
  interval       = "10m"
  delay          = "5s"
  delay_function = "constant"
  unlimited      = false
}
----
This will not schedule the deployment again if you have a single worker node.

. Define a update stanza for the job
+
[source,hcl]
----
update {
  max_parallel      = 1
  health_check      = "checks"
  healthy_deadline  = "60s" #  Default should be check_restart_grace
  min_healthy_time  = "20s" # Default should be 1 or two health check limits
  progress_deadline = "1h"
}
----
Every deployment is marked as dead if its' deployment not ready in 1h. If an allocation is started it marks as healthy it's health state is positive at least for min_healthy_time and unhealthy when it's check negative at least  healthy_deadline.

TIP: min_healthy_time should not be less then expected boot time of the service. At least the grace period.

. Define a restart stanza on group level

+
[source,hcl]
----
restart {
  # Restart if 3 of 4 check failed in check_interval
  attempts = 1
  interval = "1h"
  delay = "1s"
  mode = "fail"
}
----
+
If a healthcheck fail nomad (check_restart interval * limit is reached) nomad restarts usually the container not the allocation. The restart stanza instruct nomad to look on an interval count of restart. The example above restarts the allocation at first restart of the container within in 1h.

. Define a healthcheck with check_restart
+
[source,hcl]
----
check {
  name     = "fail_service health using http endpoint '/health'"
  port     = "http"
  type     = "http"
  path     = "/health"
  method   = "GET"
  interval = "1s"
  timeout  = "1s"
  check_restart {
    limit = 3
    grace = "15s"
    ignore_warnings = false
   }
  }
----

. The check above restarts an allocation ( if a new one should create depends on the configuration of the restart stanza) if a check fails in the interval range limit times. In the start phase of the allocation failed checks ignored in the  grace time.

. Kill timeouts and Killsignal
+
[source,hcl]
----
job "docs" {
  group "example" {
    task "server" {
      # ...
      kill_timeout = "45s" # Wait for kill
      kill_signal = "SIGINT"  # Specifies a configurable kill signal for a task "SIGINT or SIGTERM is default for docker and CTRL_BREAK_EVENT  for raw_exec
    }
  }
}
----

For playing with update strategies you can use test_deployments/fail-service_healtlhy_to_unhealtly_.nomad

=== Links
[[_200_link_nomad_task_init,nomad task dependencies]]https://developer.hashicorp.com/nomad/tutorials/task-deps/task-dependencies-interjob[Express Inter-job Dependencies with Init Tasks]

[[_200_link_nomad_hcl_lang,nomad hcl lang functions]]https://developer.hashicorp.com/nomad/docs/job-specification/hcl2[Nomad hcl lang functions]

[[_200_link_nomad_hcl_lang,nomad defaults]]https://medium.com/@obenaus.thomas/a-good-default-nomad-job-template-ea448b8a8cdd[A Good, Default Nomad Job Template]

[[_200_link_nomad_hcl_lang,nomad defaults]]https://medium.com/@obenaus.thomas/a-good-default-nomad-job-template-ea448b8a8cdd[A Good, Default Nomad Job Template]

[[_200_link_nomad_memory,Oversubscribe Memory]]https://developer.hashicorp.com/nomad/api-docs/operator/scheduler#update-scheduler-configuration[Oversubscribe Memory]


