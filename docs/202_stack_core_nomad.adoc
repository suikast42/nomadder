=== Nomad Reference Architecture
https://developer.hashicorp.com/nomad/tutorials/enterprise/production-reference-architecture-vm-with-consul#ra

=== Nomad Deployment Guide
https://developer.hashicorp.com/nomad/tutorials/enterprise/production-deployment-guide-vm-with-consul

=== Networking
In nomad client config is a `network` stanza.
You can bin there some interfaces to a named physical or virtual betwork on your host.

.Bind three named networks
[source,hcl]
----
 host_network "public" {
   interface = "eth0"
   #cidr = "203.0.113.0/24"
   #reserved_ports = "22,80"
}

host_network "private" {
    interface = "eth1"
}

host_network "default" {
    interface = "eth1"
}
----

After that you can bind in a job stanza the service to a specific interface

.Bind service to specific interface. For example make http availiable only through private network
[source,hcl]
----
  network {
    port "http-priv" {
      static       = 80
      host_network = "private"
    }
    port "https-priv" {
      static       = 443
      host_network = "private"
    }

    port "https-pub" {
      static       = 443
      host_network = "public"
    }
  }
----

==== Networking with consul connect
Consul connect proxies a port of a service. The communication to a service from an external system is encrypted and observed and can be restricted by acl policies. If a service should be protected by consul connect ( keycloak and postgres for example) then you must follow the following rules:

. The bound IP must not be accessed from public network.
. The proxied service must not be accessed from outside the host.
. If the service exposes metrics to prometheus then use the connect expose mechanism

[source,hcl]
----
job "whoami" {
  datacenters = ["nomadder1"]

  group "whoami" {
    count = 3
    network {
      mode = "bridge"
      port "web" {
        to = 8080
        # The address_mode = "alloc" is used but nomad
        # exposes the bin address to its api
        # for this reason we bind the ip to localhost
        # Then nomad exposes 127.0.0.1 zto its api and the
        # protected service is not accessible from outside of the host
        # See https://github.com/hashicorp/nomad/issues/12256
        host_network = "local"
      }
      port "health" {
        to = -1
      }
    }

    service {
      name = "whoami"
      port = "8080"
      # Register the service with the container address
      # This is only accessible from inside the host
      address_mode = "alloc"
      connect {
        sidecar_service {
          proxy {
            # The health is exposed over the host ip
            expose {
              path {
                path            = "/health"
                protocol        = "http"
                local_path_port = 8080
                listener_port   = "health"
              }
            }
          }
        }
      }
      tags = [
        "traefik.enable=true",
        "traefik.consulcatalog.connect=true",
        "traefik.http.routers.whoami.tls=true",
        "traefik.http.routers.whoami.rule=Host(`whoami.cloud.private`)",
      ]

      check {
        name     = "whoami_health"
        type     = "http"
        path     = "/health"
        port     = "web"
        interval = "10s"
        timeout  = "2s"
        address_mode = "alloc"
      }
    }

    task "whoami" {
      driver = "docker"
      config {
        image = "traefik/whoami"
        ports = ["web"]
        args  = ["--port", "${NOMAD_PORT_web}"]
      }

      resources {
        cpu    = 100
        memory = 128
      }
    }
  }
}
----


=== Memory and cpu limits
There are two main types of setting resources in nomad. The first in the nomad client configuration and the second is inside in the job configuration.

The resource configuration in the client configuration reserves memory and cpu for operating nomad. after this reservation nomad subtracts this from the available resources for job scheduling.

The resource configuration in the job configuration limits the job itself. If you don't enable https://developer.hashicorp.com/nomad/docs/job-specification/resources#memory-oversubscription[Memory Oversubscription] then the memory you defined is the min and max limit at the same time.

=== Job resiliency configuration

There are several mechanisms in nomad to control the schedule errors.

==== reschedule
job -> reschedule  | job -> group -> reschedule
This handles the case where the specified number of restarts have been attempted and the task still isn’t running. This suggests the issue could be with the Nomad client such as a hardware failure or kernel deadlock. The reschedule stanza is used to specify details for rescheduling a failing task to another nomad client. For example, reschedule the task group an unlimited number of times and increase the delay between subsequent attempts exponentially, with a starting delay of 30 seconds up to a maximum of 1 hour.

==== restart
job -> group -> restart | job -> group -> task -> restart

Specifies strategy for Nomad to restart failed tasks on the same nomad client. For example, if the application server has crashed, attempt 2 restarts within 30 minutes, delay 15s between each restart, and don’t try anymore restarts after those are exhausted.

* `delay` - Instructs the client to wait until another interval before restarting the task.

* `fail`- Instructs the client not to attempt to restart the task once the number of attempts have been used. This is the default behavior. This mode is useful for non-idempotent jobs which are unlikely to succeed after a few failures. The allocation will be marked as failed and the scheduler will attempt to reschedule the allocation according to the reschedule block.

==== check
job -> group -> service -> check |
job -> group -> task -> service -> check

The check block instructs Nomad to register a check associated with a service into the Nomad or Consul service provider.

- if connect enabled and the service needs to expose metrics then use `expose`. This delegated the metrics checks to the proxies service
- set `address_mode` to `driver` if connect is enabled

==== check_restart
job -> group -> task -> service -> check_restart | job -> group -> task -> service -> check -> check_restart
Specifies how Nomad should restart a task that is not yet failing, but has become unresponsive or otherwise unhealthy. Works together with Consul health checks. Nomad restarts tasks when a health check has failed. For example, restart the Redis task after its health check has failed 3 consecutive times, and wait 90 seconds after restarting the task to resume health checking.

==== update
job -> update  | job -> group -> update
Specifies update strategy Nomad uses when deploying a new version of the task group. i.e. when nomad job run path/to/jobspec is run. For example, perform rolling updates 3 at a time and wait until all tasks for an allocation are running and their Consul health checks are passing for at least 10 seconds before considering the allocation healthy.

==== migrate
When a Nomad client needs to come out of service, it gets marked for draining and tasks will no longer be scheduled on it. Then Nomad will migrate all existing jobs to other clients. The migrate stanza specifies the strategy for migrating tasks off of draining nodes. For example, migrate one allocation at a time, and mark migrated allocations healthy once all their tasks are running and associated health checks are passing for 10 seconds or more within a 5 minute deadline.


==== Wrap it up

. Define a reschedule for whole job.
+
.reschedule for job definition unlimited
[source,hcl]
----
reschedule {
  delay          = "10s"
  delay_function = "constant"
  unlimited      = true
}
----
This instructs nomad to try infinite schedule a failed task on the same node.
If a task have a dynamic volume or is stales you can set another  limit for instruct the scheduler to schedule the task on another node.

+
.reschedule job on another node failed deployment when more than 3 times failed
[source,hcl]
----
reschedule {
  attempts       = 3
  interval       = "10m"
  delay          = "5s"
  delay_function = "constant"
  unlimited      = false
}
----
This will not schedule the deployment again if you have a single worker node.

. Define a update stanza for the job
+
[source,hcl]
----
update {
  max_parallel      = 1
  health_check      = "checks"
  healthy_deadline  = "60s" #  Default should be check_restart_grace
  min_healthy_time  = "20s" # Default should be 1 or two health check limits
  progress_deadline = "1h"
}
----
Every deployment is marked as dead if its' deployment not ready in 1h. If an allocation is started it marks as healthy it's health state is positive at least for min_healthy_time and unhealthy when it's check negative at least  healthy_deadline.

TIP: min_healthy_time should not be less then expected boot time of the service. At least the grace period.

. Define a restart stanza on group level

+
[source,hcl]
----
restart {
  # Restart if 3 of 4 check failed in check_interval
  attempts = 1
  interval = "1h"
  delay = "1s"
  mode = "fail"
}
----
+
If a healthcheck fail nomad (check_restart interval * limit is reached) nomad restarts usually the container not the allocation. The restart stanza instruct nomad to look on an interval count of restart. The example above restarts the allocation at first restart of the container within in 1h.

. Define a healthcheck with check_restart
+
[source,hcl]
----
check {
  name     = "fail_service health using http endpoint '/health'"
  port     = "http"
  type     = "http"
  path     = "/health"
  method   = "GET"
  interval = "1s"
  timeout  = "1s"
  check_restart {
    limit = 3
    grace = "15s"
    ignore_warnings = false
   }
  }
----

. The check above restarts an allocation ( if a new one should create depends on the configuration of the restart stanza) if a check fails in the interval range limit times. In the start phase of the allocation failed checks ignored in the  grace time.

. Kill timeouts and Killsignal
+
[source,hcl]
----
job "docs" {
  group "example" {
    task "server" {
      # ...
      kill_timeout = "45s" # Wait for kill
      kill_signal = "SIGINT"  # Specifies a configurable kill signal for a task "SIGINT or SIGTERM is default for docker and CTRL_BREAK_EVENT  for raw_exec
    }
  }
}
----

For playing with update strategies you can use test_deployments/fail-service_healtlhy_to_unhealtly_.nomad

==== Linux system signals
. SIGHUP - Hangup signal. Sent to a process when its controlling terminal or session ends. Often used to reload configuration files.

. SIGINT - Interrupt signal. Sent to a process when the user presses the interrupt key combination (usually Ctrl+C) in the terminal. Often used to stop a program that is running in an infinite loop or not responding.

. SIGQUIT - Quit signal. Sent to a process when the user presses the quit key combination (usually Ctrl+) in the terminal. Often used to generate a core dump for debugging purposes.

. SIGILL - Illegal instruction signal. Sent to a process when it attempts to execute an illegal instruction.

. SIGTRAP - Trap signal. Sent to a process when a debugger or other tracing utility sets a breakpoint.

.  - Abort signal. Sent to a process by the abort() library function.

. SIGBUS - Bus error signal. Sent to a process when it attempts to access memory that cannot be accessed.

. SIGFPE - Floating point exception signal. Sent to a process when it attempts to execute an invalid arithmetic operation.

. SIGKILL - Kill signal. Sent to a process to terminate it immediately, regardless of whether it is hung or unresponsive.

. SIGUSR1 - User-defined signal 1. Can be used for any purpose that a program may require.

. SIGSEGV - Segmentation fault signal. Sent to a process when it attempts to access memory that it does not have permission to access.

. SIGUSR2 - User-defined signal 2. Can be used for any purpose that a program may require.

. SIGPIPE - Broken pipe signal. Sent to a process when it attempts to write to a pipe that has been closed.

. SIGALRM - Alarm signal. Sent to a process when the specified time interval has elapsed.

. SIGTERM - Termination signal. Sent to a process to request that it terminate gracefully.

. SIGSTKFLT - Stack fault signal. Sent to a process when it exceeds its stack size.

. SIGCHLD - Child process signal. Sent to a process when a child process terminates.

. SIGCONT - Continue signal. Sent to a process to resume it after it has been stopped.

. SIGSTOP - Stop signal. Sent to a process to stop it immediately, but without terminating it.

. SIGTSTP - Terminal stop signal. Sent to a process when the user presses the suspend key combination (usually Ctrl+Z) in the terminal.

. SIGTTIN - Terminal input signal. Sent to a process that is attempting to read input from the terminal while it is in the background.

. SIGTTOU - Terminal output signal. Sent to a process that is attempting to write output to the terminal while it is in the background.

. SIGURG - Urgent data signal. Sent to a process when urgent data is available on a socket.

. SIGXCPU - CPU time limit exceeded signal. Sent to a process when it exceeds its allotted CPU time.

. SIGXFSZ - File size limit exceeded signal. Sent to a process when it exceeds its allotted file size.

=== Nomad CPU Allocation: MHz to vCPU Conversion

Nomad's use of MHz (Megahertz) for CPU allocation is intentional. It models the CPU as a unit of raw processing bandwidth rather than abstract cores. This allows for incredibly fine-grained scheduling and properly accounts for nodes with different clock speeds and modern core architectures (like Intel's P-cores and E-cores).

To calculate your vCPU needs from a Nomad MHz allocation, you must first establish a definition for what one vCPU represents on your underlying physical hardware.

==== 1. The Core Conversion Formula

The fundamental conversion is based on the average clock speed of a single core (or the logical equivalent of a vCPU) on your Nomad client machines:

image::nomad_vcpu_1.png[]

==== 2. Step-by-Step Calculation Guide

This process ensures you translate the requested bandwidth (MHz) into a predictable unit of core capacity (vCPU equivalent).

===== Step A: Determine the Baseline (Clock Speed of One Core)

You need to know the clock speed of the physical cores on the machines running the Nomad client. This establishes your denominator.

1 Physical Core = X MHz (The effective frequency of one core).

Example: If your physical servers have cores running at a base clock speed of 2.5 GHz, this translates to 2,500 MHz.

Tip: Use the base clock speed to ensure you are reserving resources that are always available, regardless of burst or turbo frequencies.

===== Step B: Determine the Total Available Bandwidth (cpu.totalcompute)

The Nomad client agent automatically calculates the total available CPU bandwidth of the entire node and reports it as the cpu.totalcompute attribute (in MHz).

image::nomad_vpcu2.png[]

Tip: You can check this value directly on a client node by running nomad node status <node-id> and looking at the Client Attributes under the cpu section.

===== Step C: Calculate the vCPU Equivalent

Use the formula from Section 1 to perform the conversion using your baseline core speed.

[cols="1,2,2"]
|===
|Term |Value |Calculation

|Nomad Request (A)
|cpu = 4000 (4000 MHz)
|

|Core Speed (B)
|2500 MHz (2.5 GHz)
|

|vCPU Equivalent
|
|stem:[\frac{4000 \text{ MHz}}{2500 \text{ MHz/vCPU}} = 1.6 \text{ vCPUs}]
|===

In this example, an allocation requesting 4000 MHz is equivalent to 1.6 vCPUs on a host with 2.5 GHz cores.

==== 3. Consideration: Hyperthreading/SMT

While the term "vCPU" often implies a Logical Core (Hyperthread), Nomad's MHz allocation maps to time slices of the CPU bandwidth.

If you treat 1 vCPU=1 Physical Core (Full Performance), the calculation provides the required fraction of full physical cores.

Even with Hyperthreading enabled, the MHz value represents the total available capacity. A 4000 MHz request means:

The job will receive CPU time equivalent to 1.6 full 2.5 GHz cores.

Nomad handles the scheduling and CPU isolation via cgroups, which manage time shares based on the requested MHz value, ensuring the job gets its proportional slice of the total host compute.

=== Links
[[_200_link_nomad_task_init,nomad task dependencies]]https://developer.hashicorp.com/nomad/tutorials/task-deps/task-dependencies-interjob[Express Inter-job Dependencies with Init Tasks]

[[_200_link_nomad_hcl_lang,nomad hcl lang functions]]https://developer.hashicorp.com/nomad/docs/job-specification/hcl2[Nomad hcl lang functions]

[[_200_link_nomad_hcl_lang,nomad defaults]]https://medium.com/@obenaus.thomas/a-good-default-nomad-job-template-ea448b8a8cdd[A Good, Default Nomad Job Template]

[[_200_link_nomad_hcl_lang,nomad defaults]]https://medium.com/@obenaus.thomas/a-good-default-nomad-job-template-ea448b8a8cdd[A Good, Default Nomad Job Template]

[[_200_link_nomad_memory,Oversubscribe Memory]]https://developer.hashicorp.com/nomad/api-docs/operator/scheduler#update-scheduler-configuration[Oversubscribe Memory]


